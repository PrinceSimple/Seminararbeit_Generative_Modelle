@online{bcon18,
    title        = {The Next Leap: How A.I. will change the 3D industry},
    year         = {2018},
    organization = {Blender Conference},
    author       = {Andrew Price},
    url          = {https://www.youtube.com/watch?v=FlgLxSLsYWQ}
}

@article{bro19,
    title   = {A {Gentle} {Introduction} to {Generative} {Adversarial} {Networks} ({GANs})},
    url     = {https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/},
    urldate = {2020-06-19},
    author  = {Brownlee, Jason},
    year    = {2019},
    note    = {abgerufen am 16.06.2020},
    file    = {A Gentle Introduction to Generative Adversarial Networks (GANs):C\:\\Users\\Frank\\Zotero\\storage\\L96M8B5V\\what-are-generative-adversarial-networks-gans.html:text/html}
}

@article{dblp16,
    author  = {Christian Ledig and
            Lucas Theis and
            Ferenc Huszar and
            Jose Caballero and
            Andrew P. Aitken and
            Alykhan Tejani and
            Johannes Totz and
            Zehan Wang and
            Wenzhe Shi},
    title   = {Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial
            Network},
    journal = {CoRR},
    volume  = {abs/1609.04802},
    year    = {2016},
    url     = {http://arxiv.org/abs/1609.04802}
}

@article{doe16,
title    = {Tutorial on {Variational} {Autoencoders}},
abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
urldate  = {2020-06-08},
journal  = {arXiv:1606.05908 [cs, stat]},
author   = {Doersch, Carl},
year     = {2016},
url      = {http://arxiv.org/abs/1606.05908},
file     = {arXiv Fulltext PDF:C\:\\Users\\Frank\\Zotero\\storage\\YHWCAK8N\\Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Frank\\Zotero\\storage\\UF72M6VB\\1606.html:text/html}
}

@book{fos19,
    title     = {Generative deep learning: teaching machines to paint, write, compose, and play},
    author    = {Foster, David},
    year      = {2019},
    publisher = {O'Reilly Media}
}

@article{goo16,
title   = {{NIPS} 2016 tutorial: {Generative} adversarial networks},
journal = {arXiv preprint arXiv:1701.00160},
author  = {Goodfellow, Ian},
year    = {2016},
url     = {https://arxiv.org/abs/1701.00160.pdf}
}

@book{goodl16,
    title     = {Deep Learning},
    author    = {Goodfellow, I. and Bengio, Y. and Courville, A.},
    publisher = {MIT Press},
    year      = {2016}
}

@inproceedings{hua17,
    title     = {Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization},
    author    = {Huang, Xun and Belongie, Serge},
    booktitle = {ICCV},
    year      = {2017}
}

@inproceedings{kar19,
    author    = {Karras, Tero and Laine, Samuli and Aila, Timo},
    title     = {A Style-Based Generator Architecture for Generative Adversarial Networks},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2019},
    url       = {http://openaccess.thecvf.com/content_CVPR_2019/papers/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf}
}

@article{kiwe13,
    title   = {Auto-encoding variational bayes},
    author  = {Kingma, Diederik and Welling, Max},
    journal = {arXiv preprint arXiv:1312.6114},
    url     = {https://arxiv.org/abs/1312.6114},
    year    = {2013}
}

@article{ras18,
title    = {Grundlagen des {Machine} {Learning} – überwachtes und unüberwachtes {Lernen}},
url      = {https://plus-it.de/blog/machine-learning-ueberwachtes-vs-unueberwachtes-lernen/},
abstract = {Beim Machine Learning gibt es zwei hauptsächliche Ansätze: Überwachtes (supervised) Lernen und Unüberwachtes (unsupervised) Lernen.},
language = {de-DE},
urldate  = {2020-06-02},
author   = {Rastislav, Paluv},
year     = {2018},
journal  = {Plus IT},
note     = {abgerufen am 02.06.2020},
file     = {Snapshot:C\:\\Users\\Frank\\Zotero\\storage\\9U9QLHX8\\machine-learning-ueberwachtes-vs-unueberwachtes-lernen.html:text/html}
}

@article{roc19,
title   = {Understanding {Variational} {Autoencoders} ({VAEs}) - {Towards} {Data} {Science}},
url     = {https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73},
urldate = {2020-06-08},
journal = {Medium},
author  = {Rocca, Joseph},
year    = {2019},
note    = {abgerufen am 08.06.2020},
file    = {Understanding Variational Autoencoders (VAEs) - Towards Data Science:C\:\\Users\\Frank\\Zotero\\storage\\C5AK49JE\\understanding-variational-autoencoders-vaes-f70510919f73.html:text/html}
}

@inproceedings{san18,
    title     = {A style-aware content loss for real-time hd style transfer},
    author    = {Sanakoyeu, Artsiom and Kotovenko, Dmytro and Lang, Sabine and Ommer, Bjorn},
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    year      = {2018}
}

@book{SDT18,
    title     = {Deep Learning with Azure},
    author    = {Salvaris, M. and Dean, D. and Tok, W.H.},
    year      = {2018},
    publisher = {Apress, Berkeley, CA}
}

@article{sha18,
    title    = {Intuitively {Understanding} {Variational} {Autoencoders}},
    url      = {https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf},
    abstract = {And why they’re so useful in creating your own generative text, art and even music},
    language = {en},
    urldate  = {2020-06-10},
    journal  = {Medium},
    author   = {Shafkat, Irhum},
    year     = {2018},
    note     = {abgerufen am 10.06.2020},
    file     = {Snapshot:C\:\\Users\\Frank\\Zotero\\storage\\Q7VTA85H\\intuitively-understanding-variational-autoencoders-1bfe67eb5daf.html:text/html}
}

@inproceedings{Spi2018,
    title     = {Towards an Interpretable Latent Space: an Intuitive Comparison of Autoencoders with Variational Autoencoders},
    url       = {https://thilospinner.com/towards-an-interpretable-latent-space/},
    year      = {2018},
    booktitle = {Proceedings of the Workshop on Visualization for AI Explainability 2018 (VISxAI)},
    author    = {Spinner, Thilo and Körner, Jonas and Görtler, Jochen and Deussen, Oliver}
}

@inproceedings{zha17,
    author    = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris N.},
    title     = {StackGAN: Text to Photo-Realistic Image Synthesis With Stacked Generative Adversarial Networks},
    booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
    month     = {Oct},
    year      = {2017}
}

@inproceedings{zhu17,
    author    = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
    title     = {Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks},
    booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
    month     = {Oct},
    year      = {2017},
    url       = {http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf}
}

@inproceedings{rob19,
    title	= {Magenta Studio: Augmenting Creativity with Deep Learning in Ableton Live},
    author	= {Adam Roberts and Jesse Engel and Yotam Mann and Jon Gillick and Claire Kayacik and Signe Nørly and Monica Dinculescu and Carey Radebaugh and Curtis Hawthorne and Douglas Eck},
    year	= {2019},
    URL	= {http://musicalmetacreation.org/buddydrive/file/mume_2019_paper_2/},
    booktitle	= {Proceedings of the International Workshop on Musical Metacreation (MUME)}
}

