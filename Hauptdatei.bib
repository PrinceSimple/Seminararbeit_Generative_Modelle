
@article{dtut16,
  title    = {Tutorial on {Variational} {Autoencoders}},
  url      = {http://arxiv.org/abs/1606.05908},
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  urldate  = {2020-06-08},
  journal  = {arXiv:1606.05908 [cs, stat]},
  author   = {Doersch, Carl},
  month    = aug,
  year     = {2016},
  note     = {arXiv: 1606.05908},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file     = {arXiv Fulltext PDF:C\:\\Users\\Frank\\Zotero\\storage\\YHWCAK8N\\Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Frank\\Zotero\\storage\\UF72M6VB\\1606.html:text/html}
}

@inproceedings{gfg14,
  title     = {Generative adversarial nets},
  booktitle = {Advances in neural information processing systems},
  author    = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year      = {2014},
  pages     = {2672--2680}
}

@article{gfnips16,
  title   = {{NIPS} 2016 tutorial: {Generative} adversarial networks},
  journal = {arXiv preprint arXiv:1701.00160},
  author  = {Goodfellow, Ian},
  year    = {2016}
}

@article{o_pixel16,
  title   = {Pixel {Recurrent} {Neural} {Networks}},
  volume  = {abs/1601.06759},
  url     = {http://arxiv.org/abs/1601.06759},
  journal = {CoRR},
  author  = {Oord, Aäron van den and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  year    = {2016},
  note    = {\_eprint: 1601.06759}
}

@article{rjVAE19,
  title   = {Understanding {Variational} {Autoencoders} ({VAEs}) - {Towards} {Data} {Science}},
  url     = {https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73},
  urldate = {2020-06-08},
  author  = {Rocca, Joseph},
  year    = {2019},
  file    = {Understanding Variational Autoencoders (VAEs) - Towards Data Science:C\:\\Users\\Frank\\Zotero\\storage\\C5AK49JE\\understanding-variational-autoencoders-vaes-f70510919f73.html:text/html}
}

@article{rp_gml18,
  title    = {Grundlagen des {Machine} {Learning} – überwachtes und unüberwachtes {Lernen}},
  url      = {https://plus-it.de/blog/machine-learning-ueberwachtes-vs-unueberwachtes-lernen/},
  abstract = {Beim Machine Learning gibt es zwei hauptsächliche Ansätze: Überwachtes (supervised) Lernen und Unüberwachtes (unsupervised) Lernen.},
  language = {de-DE},
  urldate  = {2020-06-13},
  author   = {Rastislav, Paluv},
  year     = {2018},
  journal  = {Plus IT},
  note     = {Library Catalog: plus-it.de
Section: Trends \& Innovationen},
  file     = {Snapshot:C\:\\Users\\Frank\\Zotero\\storage\\9U9QLHX8\\machine-learning-ueberwachtes-vs-unueberwachtes-lernen.html:text/html}
}

@article{si18,
  title    = {Intuitively {Understanding} {Variational} {Autoencoders}},
  url      = {https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf},
  abstract = {And why they’re so useful in creating your own generative text, art and even music},
  language = {en},
  urldate  = {2020-06-14},
  journal  = {Medium},
  author   = {Shafkat, Irhum},
  month    = apr,
  year     = {2018},
  note     = {Library Catalog: towardsdatascience.com},
  file     = {Snapshot:C\:\\Users\\Frank\\Zotero\\storage\\Q7VTA85H\\intuitively-understanding-variational-autoencoders-1bfe67eb5daf.html:text/html}
}

@article{sp17,
  title      = {{PixelCNN}++: {Improving} the {PixelCNN} with {Discretized} {Logistic} {Mixture} {Likelihood} and {Other} {Modifications}},
  shorttitle = {{PixelCNN}++},
  url        = {http://arxiv.org/abs/1701.05517},
  abstract   = {PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.},
  urldate    = {2020-06-08},
  journal    = {arXiv:1701.05517 [cs, stat]},
  author     = {Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P.},
  month      = jan,
  year       = {2017},
  note       = {arXiv: 1701.05517},
  keywords   = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file       = {arXiv Fulltext PDF:C\:\\Users\\Frank\\Zotero\\storage\\5QDMNSKC\\Salimans et al. - 2017 - PixelCNN++ Improving the PixelCNN with Discretize.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Frank\\Zotero\\storage\\A4ZXC29W\\1701.html:text/html}
}
